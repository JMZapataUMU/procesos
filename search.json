[
  {
    "objectID": "Tema1.html#conceptos-básicos-y-ejemplos",
    "href": "Tema1.html#conceptos-básicos-y-ejemplos",
    "title": "2  Tema 1. Introducción a los procesos estocásticos",
    "section": "2.1 Conceptos básicos y ejemplos",
    "text": "2.1 Conceptos básicos y ejemplos\nA lo largo de este tema, vamos a fijar un espacio de probabilidad \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) y un subconjunto \\(\\mathbb{T}\\subset [0,\\infty)\\).\n\n\n\n\n\n\nDefinición\n\n\n\nUn proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) es una colección de variables aleatorias reales \\(X_t\\) definidas en el espacio de probabilidad \\((\\Omega,\\mathcal{F},\\mathbb{P})\\).\n\n\nInterpretamos \\(t\\) como el tiempo (medido en cierta unidad).\n\nSi \\(\\mathbb{T}\\) es contable (por ejemplo, \\(\\mathbb{T}=\\{0&lt;1&lt;2&lt;\\ldots\\}\\), diremos que el proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) es de tiempo discreto.\nSi \\(\\mathbb{T}\\) es un intervalo (por ejemplo, \\(\\mathbb{T}=[0,T]\\) o \\(\\mathbb{T}=[0,\\infty)\\), diremos que el proceso estocástico es de tiempo continuo.\n\nPara cada \\(t\\in\\mathbb{T}\\) tenemos una variable aleatoria \\(X_t\\). La variable aleatoria \\(X_t\\) tomará un valor numérico \\(X_t(\\omega)\\) para cada \\(\\omega\\in\\Omega\\). A los posibles valores que toma un proceso estocástico se les llama estados.\n\nSi para cada \\(t\\in \\mathbb{T}\\) la variable aleatoria \\(X_t\\) es de tipo discreto, diremos que el proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) es de estado discreto.\nSi para cada \\(t\\in \\mathbb{T}\\) la variable aleatoria \\(X_t\\) es de tipo continuo, diremos que el proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) es de estado continuo.\n\n\n\n\n\n\nTipos de procesos estocásticos\n\n\n\n\nEn lo que sigue, siempre supondremos que \\(0\\in\\mathbb{T}\\). Esta condición no es realmente restrictiva, ya que podemos desplazar el tiempo por una constante para garantizar que dicha condición se cumpla.\n\n\n\n\n\n\nDefinición\n\n\n\nDado un proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\), para cada realización \\(\\omega\\in\\Omega\\), la colección de números reales \\((X_t(\\omega))_{t\\in\\mathbb{T}}\\) se llama trayectoria del proceso.\n\n\n\nPara un proceso estocástico \\((X_{t})_{t=0,1,2,\\ldots}\\) de tiempo discreto, cada trayectoria define una sucesión de números reales \\((x_t)_{t=0,1,2,\\ldots}\\).\n\n\n\nPara un proceso estocástico \\((X_{t})_{t\\in [0,\\infty)}\\) de tiempo continuo, cada trayectoria define una función real \\(t\\mapsto x(t)\\colon [0,\\infty)\\to\\mathbb{R}\\).\n\nCada trayectoria corresponde a una observación particular de la evolución del valor de un proceso estocástico en el tiempo.\n\n\n\n\n\n\nEjemplo\n\n\n\nSupón que inviertes 100 euros en una cuenta bancaria con tipo de interés anual \\(R\\), compuesto anualmente. Es decir, si \\(X_t\\) es la cantidad de dinero en el año \\(t\\), entonces \\[\nX_t=100\\cdot(1+R)^t\\quad\\mbox{ para }t=0,1,2,\\ldots.\n\\] Supongamos también que el valor de \\(R&gt;0\\) es fijado cuando metes el dinero en la cuenta y sigue una distribución \\(\\mathrm{Exp}(1)\\).\nPara cada observación particular \\(R=r\\) de la variable \\(R\\) tendremos una trayectoria del proceso estocástico \\((X_t)_{t=0,1,2,\\ldots}\\). Podemos simular y dibujar cinco trayectorias de este proceso estocástico mediante el siguiente código en R:\n\nset.seed(112) \nnsim &lt;- 5  \nr &lt;- rexp(nsim, 1) \ntiempo &lt;- 0:10\nx &lt;- 100*(1+r[1])^tiempo\n\ncolores &lt;- rainbow(nsim)\n\nplot(tiempo, x, col=colores[1], type = \"l\", lty = 1)\n\nfor(i in 2:nsim){\n  x &lt;- 100*(1+r[i])^tiempo\n  lines(tiempo, x, col=colores[i])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\nUna moneda es lanzada reiteradas veces. En cada lanzamiento, apostamos un euro a cara, de modo que ganamos un euro si sale cara y perdemos un euro si sale cruz. De esta manera, si \\(X_t\\) es la ganancia acumulada en el lanzamiento \\(t\\), tendremos que \\(X_0=0\\) y \\[\nX_t=X_{t-1}+Y_t\\quad\\mbox{ para }t=1,2,3,\\ldots,\n\\] donde las variables \\(Y_1,Y_2,Y_3\\ldots\\) son indendientes y verfican \\(\\mathbb{P}(Y_t=1)=\\frac{1}{2}=\\mathbb{P}(Y_t=-1)\\).\n\n\nEl proceso del ejemplo anterior es un caso particular de un tipo de proceso el cual introducimos a continuación.\n\n\n\n\n\n\nDefinición\n\n\n\nSea una \\(Y_1,Y_2,Y_3,\\ldots\\) una sucesión de variables independientes de modo que \\(\\mathbb{P}(Y_t=1)=p\\) y \\(\\mathbb{P}(Y_t=1)=1-p\\). Al proceso estocástico \\((x_t)_{t=0,1,2,\\ldots}\\) con \\(X_0=0\\) y \\[\nX_t=X_{t-1}+Y_t\\quad\\mbox{ para }t=1,2,3,\\ldots,\n\\] se le llama paseo aleatorio (simple). Cuando \\(p=1/2\\) se dice se llama paseo aleatorio simétrico.\n\n\nPara cada secuencia de lanzamientos observados, tendremos una trayectoria diferente del proceso estocástico \\((X_t)_{t=0,1,2,\\ldots}\\). Podemos simular y dibujar veinte trayectorias de este proceso estocástico mediante el siguiente código en R:\n\nset.seed(145)\nnsim &lt;- 20\nnlanzamientos &lt;- 100\ny &lt;- sample(c(-1,1), size=nlanzamientos, replace=TRUE)\nx &lt;- c(0, cumsum(y))\ntiempo &lt;- 0:nlanzamientos\n\ncolores &lt;- rainbow(nsim)\n\nplot(tiempo, x, col=colores[1], type = \"l\", lty = 1, ylim=c(-20, 20))\n\nfor(i in 2:nsim){\n  y &lt;- sample(c(-1,1), size=nlanzamientos, replace=TRUE)\n  x &lt;- c(0, cumsum(y))\n  lines(tiempo, x, col=colores[i])\n}"
  },
  {
    "objectID": "Tema1.html#funciones-de-distribución-asociadas-a-un-proceso",
    "href": "Tema1.html#funciones-de-distribución-asociadas-a-un-proceso",
    "title": "2  Tema 1. Introducción a los procesos estocásticos",
    "section": "2.2 Funciones de distribución asociadas a un proceso",
    "text": "2.2 Funciones de distribución asociadas a un proceso\nDado un proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\), cada variable aleatoria \\(X_t\\) tendrá su propia distribución de probabilidad la cual puede ser discreta o continua. Observar un proceso estocástico en un solo tiempo de forma aislada no es útil para describir su comportamiento como función del tiempo, ya que los valores observados en un tiempo pueden condicionar su comportamiento en tiempos distintos. Por ejemplo, en el caso del camino aleatorio simétrico en el que apostamos un euro a cara, si hemos acumulado muchas ganancias en el pasado, la ganancia acumulada seguirá siendo alta en las siguientes jugadas, ya que partimos de una cantidad alta la cual disminuirá a lo sumo en un euro en cada lanzamiento.\nPor ese motivo, es necesario estudiar la distribución de probabilidad conjunta a lo largo de varios tiempos.\n\n\n\n\n\n\nDefinición\n\n\n\nSupongamos que \\((X_t)_{t\\in\\mathbb{T}}\\) es un proceso estocástico. Dada una sucesión finita de tiempos \\(\\{t_1&lt;t_2&lt;\\ldots&lt;t_n\\}\\subset \\mathbb{T}\\), la función \\(F_{t_1,t_2,\\ldots,t_n}\\colon\\mathbb{R}^n\\to[0,1]\\) definida por \\[\nF_{t_1,t_2,\\ldots,t_n}(x_1,x_2,\\ldots,x_n)=\\mathbb{P}(X_{t_1}\\le x_1, X_{t_2}\\le x_2,\\ldots,X_{t_n}\\le x_n),\n\\] se llama función de distribución (marginal) finito dimensional del proceso \\((X_t)_{t\\in\\mathbb{T}}\\).\n\n\nLa función de distribución finito dimensional describe el comportamiento probabilístico del proceso estocástico observado en los distintos tiempos \\(t_1,t_2,\\ldots,t_n\\). La distribución de probabilidad de un proceso está caracterizada por el conjunto de todas las distribuciones finito dimensionales. En particular, dos procesos estocásticos con las mismas funciones de distribución finito dimensionales tendrán un comportamiento probabilístico similar.\nEn el caso de que \\((X_t)_{t\\in\\mathbb{T}}\\) sea de estado discreto, entonces dicho comportamiento probabilístico puede ser descrito por medio la función puntual de probabilidad finito dimensional \\[\nP_{t_1,t_2,\\ldots,t_n}(x_1,x_2,\\ldots,x_n)=\\mathbb{P}(X_{t_1}=x_1, X_{t_2}=x_2,\\ldots,X_{t_n}= x_n).\n\\] En el caso de que \\((X_t)_{t\\in\\mathbb{T}}\\) sea de estado continuo, consideraremos la función de densidad finito dimensional \\(f_{t_1,t_2,\\ldots,t_n}(x_1,x_2,\\ldots,x_n)\\), la cual verifica \\[\n\\mathbb{P}(a_1&lt;X_{t_1}&lt;b_1,\\ldots,a_n&lt;X_{t_n}&lt;b_n)=\\int_{a_n}^{b_n}\\cdots \\int_{a_1}^{b_1}f_{t_1,t_2,\\ldots,t_n}(x_1,x_2,\\ldots,x_n) {\\mathrm dx_1}\\ldots {\\mathrm  dx_n}\n\\] para todo \\(-\\infty\\le a_i&lt;b_i\\le \\infty\\).\n\n\n\n\n\n\nDefinición\n\n\n\nDado un proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) la función media o función de medias \\(\\mu_X\\colon \\mathbb{T}\\to \\mathbb{R}\\) se define como \\[\\mu_X(t)=\\mathbb{E}(X_t).\\]\n\n\n\nPara un proceso estocástico \\((X_{t})_{t=0,1,2,\\ldots}\\) de tiempo discreto, la función media define una sucesión de números reales \\((\\mu_X(t))_{t=0,1,2,\\ldots}\\).\nPara un proceso estocástico \\((X_{t})_{t\\in [0,\\infty)}\\) de tiempo continuo, la función media define una función real \\(t\\mapsto \\mu_X(t)\\colon [0,\\infty)\\to\\mathbb{R}\\).\n\nLa función de medias da una idea de cómo el proceso estocástico se comporta en promedio a lo largo del tiempo.\n\n\n\n\n\n\nEjemplo\n\n\n\nVolvamos al ejemplo donde una moneda es lanzada reiteras veces y \\((X_t)_{t=0,1,2,\\ldots}\\) son las ganancias acumuladas al apostar un euro a cara en cada lanzamiento (paseo aleatorio simétrico). En este caso, \\(\\mu_X(0)=\\mathbb{E}(X_0)=\\mathbb{E}(0)=0\\) y, si \\(t&gt;0\\), \\[ \\mathbb{E}(X_t)=\\mathbb{E}(Y_1+Y_2+\\ldots+Y_t)=\\mathbb{E}(Y_1)+\\mathbb{E}(Y_2)+\\ldots+\\mathbb{E}(Y_t)=0, \\] donde hemos aplicado que \\(\\mathbb{E}(Y_s)=0\\) para todo \\(s\\). En definitiva, la ganancia acumulada promedio es \\(0\\) para cualquier número de lanzamientos efectuado.\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nDado un proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) la función de covarianzas \\(C_X\\colon \\mathbb{T}\\times\\mathbb{T}\\to \\mathbb{R}\\) se define como \\[ C_X(s,t)={\\mathrm  Cov}(X_s,X_t)=\\mathbb{E}\\Big((X_s-\\mu_X(s))(X_t-\\mu_X(t)\\Big)=\\mathbb{E}(X_s X_t)-\\mu_X(s)\\mu_X(t). \\] La función de varianzas \\(\\sigma_X^2\\colon \\mathbb{T}\\to \\mathbb{R}\\) se define como \\[ \\sigma_X^2(t)={\\mathrm  Var}(X_t)=C_X(t,t), \\] y la función de correlaciones \\(\\rho_X\\colon \\mathbb{T}\\times\\mathbb{T}\\to \\mathbb{R}\\) se define como \\[ \\rho_X(s,t)=\\frac{C_X(s,t)}{\\sigma_X(s)\\sigma_X(t)}. \\]\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\nConsideremos el proceso estocástico \\((X_t)_{t\\in [0,\\infty)}\\) definido como \\[\nX_t=A + B t\\quad\\mbox{ para todo }t,\n\\] donde \\(A\\) y \\(B\\) son variables aleatorias \\(N(1,1)\\) independientes. En este caso tenemos \\[\\begin{align*}\nC_X(s,t)&=\\mathbb{E}(X_s X_t)-\\mu_X(s)\\mu_X(t)\\\\\n&=\\mathbb{E}\\big((A + B t)(A + B s)\\Big)\\\\\n&=\\mathbb{E}\\big(A B + A B (s + t) + A B s t)\\Big)\\\\\n&=\\mathbb{E}(A)\\mathbb{E}(B) + \\mathbb{E}(A)\\mathbb{E}(B) (s + t) + \\mathbb{E}(A) \\mathbb{E}(B) s t\\\\\n&=1 + s + t + st.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\nVolvamos al ejemplo del paseo aleatorio simétrico \\((X_t)_{t=0,1,2,\\ldots}\\). Fijados dos números naturales \\(n,m\\) con \\(n\\le m\\), tenemos \\[\\begin{align*}\nC_X(n,m)&=\\mathbb{E}\\Big(X_n X_m\\Big)\\\\\n&=\\mathbb{E}\\Big(X_n (X_m-X_n + X_n)\\Big)\\\\\n&=\\mathbb{E}\\Big(X_n(X_m-X_n)\\Big) + \\mathbb{E}\\Big(X_n^2\\Big)\\\\\n&=\\mathbb{E}(X_n) \\mathbb{E}(X_m-X_n) + \\mathbb{E}(X_n^2)\\\\\n&=n=\\min(n,m),\n\\end{align*}\\] donde hemos usado que \\(X_n\\) y \\(X_m-X_n\\) son variables aleatorias independientes. Además, obsérvese que, como \\(X_n=Y_1+Y_2+\\ldots + Y_n\\) y las variables \\(Y_1,Y_2,\\ldots,Y_n\\) son independientes, se tiene que \\[ {\\mathrm  Var}(X_n) = {\\mathrm  Var}(Y_1^2)+{\\mathrm  Var}(Y_2^2)+\\ldots + {\\mathrm  Var}(Y_n^2)=1+1+\\ldots + 1=n, \\] y por tanto \\[ \\mathbb{E}(X_n^2)={\\mathrm  Var}(X_n) +(\\mathbb{E}(X_n))^2=n+0=n. \\]"
  },
  {
    "objectID": "Tema1.html#procesos-estacionarios-y-débilmente-estacionarios",
    "href": "Tema1.html#procesos-estacionarios-y-débilmente-estacionarios",
    "title": "2  Tema 1. Introducción a los procesos estocásticos",
    "section": "2.3 Procesos estacionarios y débilmente estacionarios",
    "text": "2.3 Procesos estacionarios y débilmente estacionarios\nLos procesos estocásticos se pueden dividir entre estacionarios (en sentido estricto), débilmente estacionarios y no estacionarios. Intuitivamente hablando, un proceso estocástico es estacionario (en sentido estricto) si su comportamiento probabilístico o distribución no cambia con el tiempo.\n\n\n\n\n\n\nDefinición\n\n\n\nUn proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) se dice que es estacionario si para toda sucesión finita de tiempos \\(t_1,t_2,\\ldots,t_n\\) y todo \\(s&gt;0\\) se verifica que los vectores aleatorios \\[\n(X_{t_1},X_{t_2},\\ldots,X_{t_n})\\quad\\mbox{ y }\\quad (X_{t_1+s},X_{t_2+s},\\ldots,X_{t_n+s})\n\\] tienen la misma función de distribución, es decir, \\[\nF_{t_1,t_2,\\ldots,t_n}(x_1,x_2,\\ldots,x_n)=F_{t_1+s,t_2+s,\\ldots,t_n+s}(x_1,x_2,\\ldots,x_n)\n\\] para cualesquiera números reales \\(x_1,x_2,\\ldots, x_n\\).\n\n\nEn particular, si un proceso es estacionario, entonces todas las variables aleatorias \\(X_t\\) del proceso tienen la misma distribución, es decir, están idénticamente distribuidas. Esto no implica que los vectores aleatorios que se pueden formar con las variables del proceso en diferentes instantes tengan todos la misma distribución.\nEn la práctica es muy útil saber si un proceso estocástico es estacionario cuando es necesario predecir el comportamiento futuro de dicho proceso. Si sabemos que el proceso es estacionario, entonces observando su comportamiento en el pasado podremos inferir información de su comportamiento en el futuro.\nA continuación introducimos una noción de estacionaridad menos exigente, la cual es también muy útil para predecir comportamientos futuros de procesos estocásticos reales.\n\n\n\n\n\n\nDefinición\n\n\n\nUn proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) se dice que es débilmente estacionario si:\n\n\\(\\mu_X(t)=\\mu_X(0)\\) para todo \\(t\\in\\mathbb{T}\\). Es decir, la función de medias es constante.\n\\(C_X(s,t)=C_X(0,t-s)\\) para todo \\(t,s\\in\\mathbb{T}\\) con \\(s\\le t\\). Es decir, la función de covarianzas depende sólo del salto entre tiempos.\n\n\n\nObsérvese que, en particular, si el proceso es débilmente estacionario tendrá función de varianzas constante.\nDado que la media \\(\\mu_X(t)\\) es determinada por la función de distribución marginal \\(F_t(x)\\), y la covarianza \\(C_X(s,t)\\) es determinada por la función de distribución marginal \\(F_{s,t}(x)\\), todo proceso estacionario será también débilmente estacionario. Sin embargo, es posible que un proceso sea débilmente estacionario pero no estacionario.\n\n\n\n\n\n\nEjemplo\n\n\n\nConsideremos el proceso estocástico \\((X_t)_{t\\in[0,\\infty)}\\) definido por \\[\nX_t=Y\\cos(U+ t),\n\\] donde \\(Y\\) y \\(U\\) son variables aleatorias independientes con \\(Y\\sim N(0,1)\\) y \\(U\\sim U[0,2\\pi]\\). Entonces \\[\n\\mu_X(t)=0,\n\\] y además, para \\(s\\le t\\), \\[\\begin{align*}\nC_X(s,t)&=\\mathbb{E}(X_s X_t)-\\mu_X(s)\\mu_X(t)\\\\\n&=\\mathbb{E}\\big((Y\\cos(U+ s))(Y\\cos(U+ t))\\Big)\\\\\n&=\\mathbb{E}(Y^2)\\mathbb{E}\\Big(\\cos(U+ s)\\cos(U+ t)\\Big)\\\\\n&=\\mathbb{E}\\Big(\\cos(U+ s)\\cos(U+ t)\\Big)\\\\\n&=\\mathbb{E}\\Big(\\tfrac{1}{2}\\cos(2U+ s + t) + \\tfrac{1}{2}\\cos(t - s)\\Big)\\\\\n&=\\mathbb{E}\\Big(\\tfrac{1}{2}\\cos(2U+ s + t)\\Big) + \\tfrac{1}{2}\\cos(t - s)\\\\\n&=\\tfrac{1}{2}\\int_0^{2\\pi} (\\cos(2u+ s + t) \\tfrac{1}{2\\pi})du +  \\tfrac{1}{2}\\cos(t - s)\\\\  \n&=0 +  \\tfrac{1}{2}\\cos(t - s)\\\\\n&=\\tfrac{1}{2}\\cos(t - s).\n\\end{align*}\\] En particular, tenemos que \\(C_X(s,t)=\\tfrac{1}{2}\\cos(t - s)=C_X(0,t-s)\\), de donde deducimos que el proceso estocástico \\((X_t)_{t\\in[0,\\infty)}\\) es débilmente estacionario."
  },
  {
    "objectID": "Tema1.html#procesos-gaussianos",
    "href": "Tema1.html#procesos-gaussianos",
    "title": "2  Tema 1. Introducción a los procesos estocásticos",
    "section": "2.4 Procesos gaussianos",
    "text": "2.4 Procesos gaussianos\nA continuación introducimos una de las familias de procesos estocásticos más importantes: los procesos gaussianos. Este tipo de procesos tienen importantes aplicaciones en Machine Learning.\nRecordemos primero la noción de variable Normal multivariante. Una propiedad importante de las variables normales multivariadas es que su función de densidad (y, por tanto, su distribución) está completamente determinada por el vector de medias y la matriz de covarianzas.\n\n\n\n\n\n\nDefinición\n\n\n\nSe dice que el vector aleatorio \\(\\vec{X}=(X_1,X_2,\\ldots,X_n)\\) sigue una distribución Normal multivariante si su función de densidad viene dada por: \\[\nf_X(\\vec{x}) = \\frac{1}{(2\\pi)^{n/2}\\,\\sqrt{|C_X|}}\n\\exp\\left\\{-\\tfrac{1}{2}(\\vec{x}-\\vec{\\mu}_X)^{T} C_X^{-1} (\\vec{x}-\\vec{\\mu}_X)\\right\\}\n\\quad \\text{para todo } \\vec{x}\\in \\mathbb{R}^n.\n\\] donde \\(\\vec\\mu_X\\) es el vector de medias de \\(\\vec{X}\\) y \\(C_X\\) es la matriz de covarianzas de \\(\\vec{X}\\).\n\n\n\n\n\n\n\n\nPropiedad\n\n\n\nSe puede probar, aunque no es inmediato, que una condición equivalente para que el vector aleatorio \\((X_1,X_2,\\ldots,X_n)\\) siga una distribución Normal multivariante es que, para cualesquiera números reales \\(a_1,a_2,\\ldots,a_n\\), la variable aleatoria real \\[\na_1 X_1 + a_2 X_2 + \\ldots + a_n X_n\n\\] sigue una distribución Normal univariante.\n\n\nOtra importante propiedad de las variables aleatorias normales multivariadas que conviene recordar es que las transformaciones lineales de estas variables también son normales multivariadas. En otras palabras, si \\(\\vec{X}\\) es un vector aleatorio Normal multivariante de tamaño n, \\(A\\) es una matriz constante de tamaño \\(m\\times n\\), y \\(\\vec{b}\\) es un vector constante de tamaño \\(m\\), entonces el vector aleatorio \\(\\vec{Y}=A\\vec{X}+\\vec{b}\\) también es un vector aleatorio Normal multivariante.\n\n\n\n\n\n\nDefinición\n\n\n\nUn proceso estocástico \\((X_t)_{t\\in\\mathbb{T}}\\) se dice que es gaussiano (o Normal) si para toda sucesión de tiempos \\(t_1,t_2,\\ldots, t_n\\in\\mathbb{T}\\) el vector aleatorio \\[(X_{t_1},X_{t_2},\\ldots,X_{t_n})\\] es una variable Normal multivariante.\n\n\nUna importante propiedad de los procesos gaussianos es que las nociones de estacionaridad y estacionaridad débil son equivalentes para este tipo de procesos. En la práctica esto significa que para verificar que un proceso gaussiano es estacionario basta analizar su media y covarianza. Concretamente, tenemos lo siguiente.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\((X_t)_{t\\in\\mathbb{T}}\\) un proceso estocástico gaussiano. Si \\((X_t)_{t\\in\\mathbb{T}}\\) es débilmente estacionario, entonces es también estacionario."
  },
  {
    "objectID": "Tema1.html#ejemplos-y-simulación",
    "href": "Tema1.html#ejemplos-y-simulación",
    "title": "2  Tema 1. Introducción a los procesos estocásticos",
    "section": "2.5 Ejemplos y simulación",
    "text": "2.5 Ejemplos y simulación\nEn lo que sigue vamos a estudiar y simular en R algunos ejemplos particulares de procesos estocásticos gaussianos, analizando en cada caso si se trata de procesos estacionarios o no estacionarios.\n\n2.5.1 Ruido blanco gaussiano\nUn proceso estocástico \\((X_{t})_{t\\in\\mathbb{T}}\\) se llama ruido blanco gaussiano si las variables aleatorias \\(X_t\\) son independientes y siguen una distribución \\(N(0,\\sigma^2)\\).\nClaramente el ruido blanco gaussiano es un proceso gaussiano. Además, \\(\\mu_X(t)=0\\), \\(C_X(t,s)=0\\) si \\(t\\neq s\\), y \\(C_X(t,t)=\\sigma^2\\). Lo cual en particular implica que el ruido blanco gaussiano es un proceso estacionario.\nPodemos simular en R un trayectoria del ruido blanco gaussiano:\n\nset.seed(11)\nsigma &lt;- 1\ntiempos = seq(from = 0, to = 1, by = 0.001)\nx=rnorm(length(tiempos), 0, sigma)\nplot(tiempos, x, type = \"l\", lty = 1)\n\n\n\n\n\n\n\n\nObtenemos una trayectoria extremadamente irregular. Además, observamos a simple vista un comportamiento estacionario del proceso.\n\n\n2.5.2 Movimiento Browniano\nUn proceso estocástico \\((X_{t})_{t\\in [0,\\infty)}\\), de tiempo continuo, se dice que es un movimiento Browniano o proceso de Wiener si:\n\n\\(X_0=0\\),\nPara todo par de tiempos \\(s\\le t\\), la variable aleatoria \\(X_{t}-X_{s}\\) sigue una distribución \\(N(0,t-s)\\),\nPara cualquier sucesión de tiempos \\(t_1&lt;t_2&lt;\\ldots&lt;t_n\\) se tiene que las variables aleatorias \\[ X_{t_2}-X_{t_1},\\quad X_{t_3}-X_{t_2},\\quad \\ldots,\\quad X_{t_n}-X_{t_{n-1}} \\] son independientes.\nLas trayectorias \\(t\\mapsto X_t(\\omega)\\) son funciones continuas.\n\nEl movimiento Browniano se utiliza en finanzas para modelar la evolución del precio de activos financieros como las acciones o los tipos de cambio. En particular, se aplica a la valoración de opciones financieras mediante el modelo de Black-Scholes, y el análisis de riesgos.\nA continuación vamos a simular una trayectoria del movimiento Browniano. Para ello fijamos un incremento de tiempo \\(\\Delta t&gt;0\\) y un número natural \\(n\\). Deseamos simular una trayectoria en los instantes de tiempo \\(t_0=0\\), \\(t_1=\\Delta t\\), …, \\(t_n=n\\Delta t\\). En vista de la definición de movimiento Browniano, tenemos que los incrementos \\[\n\\Delta X_{t_1}=X_{t_1}-X_{t_0},\\quad \\Delta X_{t_2}=X_{t_2}-X_{t_1},\\quad ...,\\quad \\Delta X_{t_n}=X_{t_n}-X_{t_{n-1}},  \n\\] son variables aleatorias idependientes y siguen todas ellas una distribución \\(N(0,\\Delta t)\\). De este modo, nuestra estrategia será simular los valores de los incrementos mediante la generación de una muestra aleatoria de \\(n\\) variables \\(N(0,\\Delta t)\\) independientes, y reconstruir la trayectoria del proceso a partir de dichos incrementos. Esto podemos lograrlo mediante el siguiente código de R:\n\nset.seed(11)\nn &lt;- 1000\ndt &lt;- 1/n\ntiempo &lt;- seq(from=0, to=1, by=dt)\nincrementos &lt;- rnorm(n, 0, sqrt(dt))\nx &lt;- c(0, cumsum(incrementos))\nplot(tiempo, x, type = \"l\", lty = 1)\n\n\n\n\n\n\n\n\nPara analizar el comportamiento probabilístico, simularemos diez trayectorias del proceso:\n\nset.seed(123)\nnsim &lt;- 10\nn &lt;- 1000\ndt &lt;- 1/n\ntiempos &lt;- seq(from=0, to=1, by=dt)\nincrementos &lt;- rnorm(n, 0, sqrt(dt))\nx &lt;- c(0, cumsum(incrementos))\n\ncolores &lt;- rainbow(nsim)\n\nplot(tiempos, x, ylim=c(-2,2), type = \"l\", lty = 1, col=colores[1])\n\n\nfor(i in 2:nsim){\n  incrementos=rnorm(n, 0, sqrt(dt))\n  x &lt;- c(0, cumsum(incrementos))\n  lines(tiempos, x, col=colores[i])\n}\n\n\n\n\n\n\n\n\nA simple vista observamos un comportamiento no estacionario del proceso. Notamos que a medida que avanza el tiempo, las trayectorias tienden a volverse más dispersas, lo que indica que el comportamiento probabilístico de las trayectorias varía con el tiempo.\nEl movimiento Browniano es un proceso gaussiano. De hecho, la distribución de probabilidad del vector aleatorio \\((X_{t_1},X_{t_2},\\ldots,X_{t_n})\\), para \\(t_1&lt;t_2&lt;\\ldots&lt;t_n\\), es normal multivariada porque se obtiene como una transformación lineal del vector \\((X_{t_1},X_{t_2}-X_{t_1},\\ldots,X_{t_n}-X_{t_{n-1}})\\), el cual es normal multivariado porque sus componentes son independientes y normales. ¿Puedes encontrar dicha transformación lineal?.\nAdemás, \\(\\mu_X(t)=\\mathbb{E}(X_t)=0\\) y, para \\(s\\le t\\), \\[\\begin{align*}\nC_X(s,t)&=\\mathbb{E}\\Big(X_s X_t\\Big)\\\\\n&=\\mathbb{E}\\Big(X_s (X_t-X_s + X_s)\\Big)\\\\\n&=\\mathbb{E}\\Big(X_s(X_t-X_s)\\Big) + \\mathbb{E}\\Big(X_s^2\\Big)\\\\\n&=s=\\min(s,t).\n\\end{align*}\\] En resumen, el movimiento Browniano es un proceso gaussiano con funciones de media y covarianza \\(\\mu_X(t)=0\\), y \\(C_X(s,t)=\\min(s,t)\\), respectivamente. En particular, observamos que este proceso no es estacionario (ni siquiera en el sentido débil), ya que en general no se verifica que \\(C_X(s,t)=C_X(0,t-s)\\). Por ejemplo, \\(C_X(3,4)=\\min(3,4)=3\\); sin embargo, \\(C_X(0,4-3)=\\min(0,1)=0\\).\nA continuación vamos a detallar otra estrategia para simular el movimiente Browniano, la cual se basa en que dicho proceso es gaussiano. Aunque nos vamos a centrar en el caso particular del movimiento Browniano, este procedimiento sirve para simular cualquier proceso gaussiano conocidas sus funciones de media y covarianza. Fijados los tiempos \\(t_1&lt;\\ldots&lt;t_n\\) sabemos que el vector aleatorio \\((X_{t_1},X_{t_2},\\ldots,X_{t_n})\\) sigue una distribución multivariada de media \\(\\mu_X=0\\) y matriz de convarianzas \\(C_X=(c_{i,j})_{n\\times n}\\) donde \\(c_{i,j}:=\\min\\{t_i,t_j\\}\\). De esta manera, podremos simular una trayectoria mediante la obtención de una muestra aleatoria del anterior vector aleatorio. Para ello utilizamos la libreria mvtnorm de R, la cual permite simular variables aleatorias normales multivariadas:\n\nset.seed(23)\nn &lt;- 1000\ndt &lt;- 1/n\ntiempo = seq(from=0, to=1, by=dt)\n\n# Definimos la matriz de covarianzas\nC &lt;- matrix(NA, nrow = n, ncol = n)\nC &lt;- dt*pmin(row(C),col(C))\n\n# Simulamos una normal multivariada \nlibrary(mvtnorm)\nx &lt;- rmvnorm(1, sigma=C)\n\n# Añadimos el valor 0 inicial\nx &lt;- c(0, x)\nplot(tiempo, x, type = \"l\", lty = 1)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Procesos gaussianos estacionarios isotrópicos\nUn tipo particular de proceso gaussiano estacionario son los procesos gaussianos estacionarios isotrópicos. Decimos que un proceso gaussiano estacionario \\((X_t)_{t\\in\\mathbb{T}}\\) es isotrópico si su función de medias es constante, \\(\\mu_X(t)=\\mu\\), y la función de covarianza depende sólo de la distancia \\(|t-s|\\), es decir,\n\\[\nC_X(s,t)=K(|t-s|)\n\\] para cierta función \\(K\\colon [0,\\infty)\\to [0,\\infty)\\) llamada núcleo.\nTomemos por ejemplo \\(\\mu_X(t)=0\\) y el núcleo exponencial cuadrático \\(K_l(x):=\\exp(-x^2/2l^2)\\), donde \\(l\\) es un parámetro de escala. Vamos a simular en R una trayectoria de dicho proceso:\n\nset.seed(112)\nn &lt;- 1000\ndt &lt;- 1/n\ntiempo &lt;- seq(from=0, to=1, by=dt)\n\n\n# Definimos la matriz de covarianzas\nl &lt;- 1\nC &lt;- matrix(NA, nrow = n+1, ncol = n+1)\nC &lt;- dt*abs(row(C)-col(C))\nC &lt;- exp(-C^2/(2*l^2))\n\n# Simulamos una normal multivariada \nlibrary(mvtnorm)\nx &lt;- rmvnorm(1, sigma=C)\nplot(tiempo, x, type = \"l\", lty = 1)\n\n\n\n\n\n\n\n\nPara analizar el comportamiento probabilístico, podemos simular diez trayectorias del proceso:\n\nset.seed(112)\nnsim &lt;- 10\nn &lt;- 1000\ndt &lt;- 1/n\ntiempo &lt;- seq(from=0, to=1, by=dt)\n\n\n# Definimos la matriz de covarianzas\nl &lt;- 1\nC &lt;- matrix(NA, nrow = n+1, ncol = n+1)\nC &lt;- dt*abs(row(C)-col(C))\nC &lt;- exp(-C^2/(2*l^2))\n\n# Simulamos una normal multivariada \nlibrary(mvtnorm)\nx &lt;- rmvnorm(1, sigma=C)\n\ncolores &lt;- rainbow(10)\n\nplot(tiempo, x, type = \"l\", lty = 1, ylim=c(-2.5,2.5), col=colores[1])\n\nfor(i in 2:nsim){\n  x &lt;- rmvnorm(1,sigma=C)\n  lines(tiempo, x, col=colores[i])\n}\n\n\n\n\n\n\n\n\nA diferencia del movimiento Browniano, vemos que las trayectorias presentan un comportamiento estacionario, ya que su distribución es la misma a lo largo del tiempo. Otra diferencia con el movimiento Browniano es que las trayectorias tienen una apariencia suave.\nPodemos cambiar el parámero de escala, tomando \\(l=0.1\\). De este modo obtenemos las trayectorias:\n\n\n\n\n\n\n\n\n\nTomemos ahora el núcleo de Ornstein–Uhlenbeck \\(K_l(x):=\\exp(-|x|/l)\\).\nSimulemos algunas trayectorias en R:\n\nset.seed(114)\nnsim &lt;- 10\nn &lt;- 1000\ndt &lt;- 1/n\ntiempo &lt;- seq(from=0, to=1, by=dt)\n\n\n# Definimos la matriz de covarianzas\nl &lt;- 1\nC &lt;- matrix(NA, nrow = n+1, ncol = n+1)\nC &lt;- dt*abs(row(C)-col(C))\nC &lt;- exp(-C/l)\n\n# Simulamos una normal multivariada \nlibrary(mvtnorm)\nx &lt;- rmvnorm(1, sigma=C)\n\ncolores &lt;- rainbow(10)\n\nplot(tiempo, x, type = \"l\", lty = 1, ylim=c(-2.5,2.5), col=colores[1])\n\nfor(i in 2:nsim){\n  x &lt;- rmvnorm(1,sigma=C)\n  lines(tiempo, x, col=colores[i])\n}\n\n\n\n\n\n\n\n\nVemos que las trayectorias tienen una apariencia más irregular la cual nos recuerda al movimiento Browniano. Sin embargo, en este caso tenemos un proceso estacionario cuyas trayectorias se comportan de manera similar a lo largo del tiempo.\n\n\n2.5.4 Movimiento Browniano fraccional\nOtro proceso gaussiano importante es el movimiento Browniano fraccional, el cual es una generalización del movimiento Browniano clásico. A diferencia de éste, los incrementos del movimiento Browniano fraccional no son independientes.\nDado un número \\(H \\in (0,1)\\), decimos que un proceso estocástico \\((X_t)_{t\\in [0,\\infty)}\\) es un movimiento Browniano fraccional con índice de Hurst \\(H\\) si es un proceso gaussiano, su función de medias es constantemente cero, \\(\\mu_X(t)=0\\), y su función de covarianzas está dada por \\[\nC_X(s,t) = \\frac{1}{2}\\left( t^{2H} + s^{2H} - |t-s|^{2H} \\right).\n\\] El índice de Hurst \\(H\\) describe la irregularidad del movimiento resultante: valores más altos producen trayectorias más suaves. Este proceso fue introducido por Mandelbrot y Van Ness (1968) para modelizar fenómenos como los mercados financieros.\n\nCuando \\(H = \\tfrac{1}{2}\\), el proceso coincide con el movimiento Browniano usual.\nCuando \\(H &gt; \\tfrac{1}{2}\\), los incrementos están positivamente correlados.\nCuando \\(H &lt; \\tfrac{1}{2}\\), los incrementos están negativamente correlados.\n\nVamos a simular en R una trayectoria de dicho proceso para H=0.2:\n\nset.seed(23)\nn &lt;- 1000\ndt &lt;- 1/n\ntiempo = seq(from=0, to=1, by=dt)\n\n# Definimos la matriz de covarianzas para H=0.2\nH &lt;- 0.2\nC &lt;- matrix(NA, nrow = n, ncol = n)\nC &lt;- (1/2)*((dt*row(C))^(2*H) + (dt*col(C))^(2*H) - abs(row(C) - col(C))^(2*H))\n\n\n# Simulamos una normal multivariada \nlibrary(mvtnorm)\nx &lt;- rmvnorm(1, sigma=C)\n\n# Añadimos el valor 0 inicial\nx &lt;- c(0, x)\nplot(tiempo, x, type = \"l\", lty = 1)\n\n\n\n\n\n\n\n\nVemos que la trayectoria obtenida menos suave que en el caso del movimiento Browniano usual. Al igual que el movimiento Browniano usual, no es un proceso estacionario."
  },
  {
    "objectID": "index.html#presentación",
    "href": "index.html#presentación",
    "title": "Procesos estocásticos y series temporales",
    "section": "0.1 Presentación",
    "text": "0.1 Presentación\nEl presente libro contiene los apuntes de la asignatura “Procesos estocásticos y series temporales”, la cual es impartida en el Grado en Ciencia e Ingeniería de Datos de la Universidad Politécnica de Cartagena y la Universidad de Murcia."
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Procesos estocásticos y series temporales",
    "section": "0.2 Licencia",
    "text": "0.2 Licencia\nEste es un libro abierto bajo la licencia\n\nPuedes copiar, redistribuir y adaptar la obra, siempre que des el crédito correspondiente, no la uses con fines comerciales y compartas bajo la misma licencia."
  },
  {
    "objectID": "Tema2.html#cadenas-de-markov-de-tiempo-discreto",
    "href": "Tema2.html#cadenas-de-markov-de-tiempo-discreto",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.1 Cadenas de Markov de tiempo discreto",
    "text": "3.1 Cadenas de Markov de tiempo discreto\n\n\n\n\n\n\nDefinición\n\n\n\nUn proceso estocástico \\((X_n)_{n=0,1,2,\\ldots}\\) de tiempo discreto se dice que es una cadena de Markov si se verifica:\n\nCada \\(X_n\\) toma valores en un conjunto numerable \\(\\mathcal{S}\\).\nSe cumple que\n\n\\[\n\\begin{aligned}\n\\mathbb{P}\\bigl(X_{n+1}=a_{n+1}\\mid X_n=a_n,X_{n-1}=a_{n-1},\\ldots, X_0=a_0\\bigr)\n&= \\mathbb{P}\\bigl(X_{n+1}=a_{n+1}\\mid X_n=a_n\\bigr)\n\\end{aligned}\n\\tag{3.1}\\]\ndonde \\(a_0,a_1,\\ldots,a_{n+1}\\in\\mathcal{S}\\).\n\n\nLa condición (Equation 3.1) arriba se llama propiedad de Markov. La interpretación es que la probabilidad de cualquier valor futuro del proceso, dado el valor actual, no está influenciada por ningún valor pasado. Se dice que las cadenas de Markov son procesos estocásticos sin memoria.\n\n\n\n\n\n\nEjemplo (PageRank)\n\n\n\nPageRank es un algoritmo creado y desarrollado por la compañía tecnológica estadounidense Google para ordenar las apariciones de las páginas en cada búsqueda, dando preferencia a aquellas páginas que sean más “importantes” o “populares”. Para medir esto, se analiza la cadena de Markov resultante de un individuo o “surfeador de la web” que va pulsando links al azar en un conjunto de páginas de internet. Por ejemplo, supongamos que el surfeador de la web navega haciendo clics al azar en las páginas A,B,C,D donde:\n\nA tiene enlace a B,\nB tiene enlaces a A y C,\nC tiene enlace a A,\nD tienes enlaces a las otras tres páginas.\n\nEste proceso da lugar a una cadena de Markov con espacio de estados \\(\\{A,B,C,D\\}\\), el cuál puede ser descrito mediante el siguiente grafo.\n\n\n\n\n\n\n\n\n\nAdemás, tenemos las siguientes probabilidades de transición entre estados.\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n0%\n100%\n0%\n0%\n\n\nB\n50%\n0%\n50%\n0%\n\n\nC\n100%\n0%\n0%\n0%\n\n\nD\n33.3333%\n33.3333%\n33.3333%\n0%\n\n\n\nClaramente este proceso da lugar a una cadena de Markov, ya que, en cada paso, las probabilidades de visitar una página u otra, sólo depende de en qué página se encuentre el surfeador, sin importar qué páginas haya visitado anteriormente.\nEl algoritmo PageRank trata de determinar la probabilidad con la que una página es visitada a medida que el surfeador hace más y más clics, considerando como más importantes aquellas páginas para las que esta probabilidad sea mayor. Éste es el criterio usado para ordenar las páginas en cada búsqueda en Google. Analizaremos este problema en detalle al final del tema.\n\n\n\n\n\n\n\n\nEjemplo (Paseo aleatorio)\n\n\n\nEl paseo aleatorio simétrico \\((X_n)_{n=0,1,2,\\ldots}\\) es una cadena de Markov con espacio de estados infinito\n\\[\n\\mathcal{S}=\\mathbb{Z}={\\ldots,-3,-2,-1,0,1,2,3,\\ldots}.\n\\]\nEn este caso, para todo \\(i\\in\\mathcal{S}\\)\n\\[\n\\mathbb{P}(X_{n+1}=i+1\\mid X_n=i)=\\mathbb{P}(X_{n+1}=i-1\\mid X_n=i)=\\frac{1}{2},\n\\]\n\\[\n\\mathbb{P}(X_{n+1}=j\\mid X_n=i)=0\\quad\\mbox{ si }j\\neq i\\pm 1.\n\\]\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSupongamos que \\((X_n)_{n=0,1,2,\\ldots}\\) es una cadena de Markov con espacio de estados \\(\\mathcal{S}\\).\nLas son la probabilidades \\[\np_{x,y}(n)=\\mathbb{P}(X_n=y\\mid X_{n-1}=x),\n\\] donde \\(x,y\\in\\mathcal{S}\\) y \\(n=1,2,3,\\ldots\\).\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nDecimos que la cadena de Markov \\((X_n)_{n=0,1,2,\\ldots}\\) es homogénea si las probabilidades de transición no dependen del tiempo. En tal caso, definimos \\[\\begin{align*}\np_{x,y}=&\\mathbb{P}(X_n=y|X_{n-1}=x)\\\\\n=&\\mathbb{P}(X_1=y|X_{0}=x)\n\\end{align*}\\] donde hemos eliminado \\(n\\) de la notación.\n\n\nEn el resto de este tema vamos a trabajar con cadenas de Markov homogéneas con un número de estados finito. Por lo que a partir de ahora, cada vez que nos refiramos a una cadena estaremos hablando de una cadena de Markov con esas características.\nEn general, usaremos que \\((X_n)_{n=0,1,2,\\ldots}\\) es una cadena con espacio de estados finito que denotaremos por \\(\\mathcal{S}=\\{1,2,3,\\ldots,N\\}\\).\n\n\n\n\n\n\nDefinición\n\n\n\nDefinimos la matriz de transición de \\((X_n)_{n=0,1,2,\\ldots}\\) como\n\\[\nP=(p_{i,j})_{i,j=1,2,\\ldots,N}= \\left[\n\\begin{array}{cccc}\np_{1,1} & p_{1,2} & \\ldots & p_{1,N}\\\\\np_{2,1} & p_{2,2} & \\ldots & p_{2,N}\\\\\n\\ldots & \\ldots  & \\ldots & \\ldots\\\\\np_{N,1} & p_{N,2} & \\ldots & p_{N,N}\\\\\n\\end{array}\n\\right]\n\\]\n\n\nObsérvese que las entradas de la fila 1 describen todas las probabilidades posibles condicionadas a empezar en el estado \\(i=1\\). Por tanto, la suma de todas ellas debe ser igual a \\(1\\). Obviamente, lo mismo es cierto para el resto de filas, es decir, \\(p_{i,1}+p_{i,2}+\\ldots+p_{i,N}=1\\) para cada \\(i\\). Por tanto, en una matriz de transición, la suma de todos los elementos de cualquier fila debe de ser 1. Sin embargo, esta condición no tiene por qué verificarse para las columna de una matriz de transición.\n\n\n\n\n\n\nEjemplo (Urnas de Ehrenfest)\n\n\n\nSupongamos que tenemos dos urnas, \\(U_1\\) y \\(U_2\\). En ellas están distribuidas \\(N\\) bolas numeradas. En cada paso, se elige un número al azar entre \\(\\{1,2,\\ldots,N\\}\\). A continuación se observa en qué urna está la bola con el número elegido y se cambia de urna. Denotemos por \\(X_n\\) el número de bolas contenidas en la urna \\(U_1\\) en tiempo \\(n\\). De esta manera, definimos una cadena \\((X_n)_{n=0,1,2,\\ldots}\\) con espacio de estados \\(\\mathcal{S}=\\{0,1,2,\\ldots,N\\}\\) y probabilidades de transición \\[\\begin{align*}\n\\mathbb{P}&(X_n=i+1|X_{n-1}=i)=\\frac{N-i}{N},\\\\\n\\mathbb{P}&(X_n=i-1|X_{n-1}=i)=\\frac{i}{N},\\\\\n\\mathbb{P}&(X_n=j|X_{n-1}=i)=0\\quad\\mbox{ si }j\\neq i\\pm 1.\n\\end{align*}\\] Por ejemplo, si \\(N=3\\) tenemos espacio de estados \\(\\{0,1,2,3\\}\\) y matriz de transición \\[\nP =\\left[\n  \\begin{array}{cccc}\n0 & 1 & 0 & 0\\\\\n1/3 & 0 & 2/3 & 0\\\\\n0 & 2/3  & 0 & 1/3\\\\\n0 & 0 & 1 & 0\\\\\n  \\end{array}\n  \\right].\n\\]\n\n\n\n\n\n\n\n\nEjemplo (Ruina del jugador)\n\n\n\nConsideremos un individuo que juega a la ruleta, que posee una riqueza inicial de \\(X_0\\) euros, y que apuesta 1 euro a rojo en cada jugada con probabilidad de ganar \\(p\\). El jugador seguirá apostando hasta que, o bien alcance una riqueza objetivo \\(M\\), o bien hasta que se arruine. El proceso \\((X_n)_{n=0,1,2,\\ldots}\\) de la riqueza acumulada hasta la jugada \\(n\\) es una cadena de Markov con espacio de estados finito \\(\\mathcal{S}=\\{0,1,\\ldots,M\\}\\).\n\n\n\n\n\n\n\n\n\nEn este caso, el proceso de la fortuna acumulada \\((X_n)_{n=0,1,2,\\ldots}\\) es una cadena con espacio de estados \\(\\{0,1,2,\\ldots,M\\}\\) y matriz de transición\n\\[\nP = \\left[\n  \\begin{array}{ *{8}{c} }\n      1 &  0  &  0  & 0  & \\ldots & 0 & 0 & 0\\\\\n    1-p &  0  &  p  & 0  & \\ldots & 0 & 0 & 0 \\\\\n     0  & 1-p &  0  & p  & \\ldots & 0 & 0 & 0 \\\\\n     \\vdots  &  \\vdots  & \\vdots & \\vdots  & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n     0  &  0  & 0 & 0  & \\ldots & 0 & p & 0 \\\\\n     0  &  0  & 0 & 0  & \\ldots & 1-p & 0 & p \\\\\n     0  &  0  &  0  & 0 &  \\ldots & 0 & 0 & 1 \\\\\n  \\end{array}\n  \\right].\n\\]"
  },
  {
    "objectID": "Tema2.html#matriz-de-transición-de-n-pasos",
    "href": "Tema2.html#matriz-de-transición-de-n-pasos",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.2 Matriz de transición de n pasos",
    "text": "3.2 Matriz de transición de n pasos\nEn esta sección, sea \\((X_n)_{n=0,1,2,\\ldots}\\) una cadena con espacio de estados \\(\\mathcal{S}=\\{1,2,\\ldots,N\\}\\). En general, definimos lo siguiente.\n\n\n\n\n\n\nDefinición\n\n\n\nDados dos estados \\(i,j\\in\\mathcal{S}\\), definimos la probabilidad de transición de \\(n\\) pasos\n\\[\np_{i,j}^{(n)}=\\mathbb{P}(X_n=j\\mid X_{0}=i).\n\\]\nDefinimos la matriz de transición de \\(n\\) pasos de \\((X_n)_{n=0,1,2,\\ldots}\\) como\n\\[\nP^{(n)}=\\Big(p_{i,j}{(n)}\\Big)_{i,j=1,2,\\ldots,N}= \\left[\n\\begin{array}{cccc}\np_{1,1}^{(n)} & p_{1,2}^{(n)} & \\ldots & p_{1,N}^{(n)}\\\\\np_{2,1}^{(n)} & p_{2,2}^{(n)} & \\ldots & p_{2,N}^{(n)}\\\\\n\\ldots & \\ldots  & \\ldots & \\ldots\\\\\np_{N,1}^{(n)} & p_{N,2}^{(n)} & \\ldots & p_{N,N}^{(n)}\\\\\n\\end{array}\n\\right].\n\\]\n\n\nPara entender cómo calcular la matriz de transición de n pasos analicemos el siguiente ejemplo. Consideremos la cadena \\((X_n)_{n=0,1,2,\\ldots}\\) dada por el grafo:\n\n\n\n\n\n\n\n\n\nEsta cadena tiene matriz de transición\n\\[\nP= \\left[\n\\begin{array}{cccc}\n1/2 & 1/2 & 0 & 0\\\\\n1/3 & 0 & 2/3 & 0\\\\\n1/2 & 0 & 0 & 1/2\\\\\n0 & 0 & 0 & 1\n\\end{array}\n\\right]\n\\]\nQueremos ver lo que ocurre tras dos pasos del proceso.\nPor ejemplo, veamos la probabilidad de llegar al estado 1 desde el estado 1 en dos pasos\n\\[\np_{1,1}^{(2)}=\\mathbb{P}(111) + \\mathbb{P}(121)=p_{1,1}\\cdot p_{1,1} + p_{1,2}\\cdot p_{2,1}=\\tfrac{1}{2}\\cdot\\tfrac{1}{2} + \\tfrac{1}{2}\\cdot\\tfrac{1}{3}=\\tfrac{5}{12}\n\\]\nIncluyendo todos los posibles caminos, incluso aquellos que no existen en el grafo, podemos poner\n\\[\n\\begin{aligned}\np_{1,1}^{(2)}&=\\mathbb{P}(111) + \\mathbb{P}(121)+\\mathbb{P}(131)+\\mathbb{P}(141)\\\\\n&=p_{1,1}\\cdot p_{1,1} + p_{1,2}\\cdot p_{2,1} + p_{1,3}\\cdot p_{3,1} + p_{1,4}\\cdot p_{4,1}\\\\\n&=\\tfrac{1}{2}\\cdot \\tfrac{1}{2} + \\tfrac{1}{2}\\cdot \\tfrac{1}{3} + 0 + 0=\\tfrac{5}{12}.\n\\end{aligned}\n\\]\nEscrito de esta manera, vemos que \\(p_{1,1}^{(2)}\\) es la primera entrada de la matriz \\(P^2=P\\cdot P\\). Es más, tenemos que \\(P^2=(p^{(2)}_{i,j})_{i,j\\in\\{1,2,3,4\\}}\\).\nEn general, tenemos lo siguiente.\n\n\n\n\n\n\nTeorema\n\n\n\nLa matriz de transición de \\(n\\) pasos vendrá dada por \\(n\\)-ésima potencia de \\(P\\). Es decir,\n\\[\nP^n=(p_{i,j}^{(n)})_{i,j=1,2,\\ldots,N}.\n\\]\n\n\nComo consecuencia del teorema anterior, se tiene la relación:\n\\[\n\\pi^{(n)}=\\pi{(0)} \\cdot P^n\n\\]\ndonde:\n\\[\n\\pi^{(0)}=(\\pi{(0)}_1, ...,\\pi^{(0)}_N)\n\\]\ndenota el vector de probabilidades iniciales de cada estado, es decir, \\(\\pi^{(0)}_j=\\mathbb{P}(X_0=j)\\) y\n\\[\n\\pi^{(n)}=(\\pi{(n)}_1, ...,\\pi^{(n)}_N)\n\\] denota el vector de probabilidades de cada estado en el instante \\(n\\), es decir, \\(\\pi^{(n)}_j=\\mathbb{P}(X_n=j)\\), para todo \\(j=1,2,...,N\\).\nTeniendo en cuenta el teorema anterior junto al hecho de que \\(P^{m+n}=P^m P^n\\), tenemos lo siguiente.\n\n\n\n\n\n\nCorolario (Ecuaciones de Chapman-Kolmogorov)\n\n\n\n\\[\np_{i,j}^{(n+m)}=\\sum_{k=1}^N p_{i,k}^{(n)} p_{k,j}^{(m)}.\n\\]"
  },
  {
    "objectID": "Tema2.html#clasificación-de-los-estados",
    "href": "Tema2.html#clasificación-de-los-estados",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.3 Clasificación de los estados",
    "text": "3.3 Clasificación de los estados\nSea \\((X_n)_{n=0,1,2,\\ldots}\\) una cadena con espacio de estados \\(\\mathcal{S}\\). Dados dos estados \\(x,y\\in\\mathcal{S}\\), definimos \\[ r_{x,y}=\\mathbb{P}(X_n=y\\mbox{ para algún }n\\ge 1\\mid X_0=x). \\] Esto es, \\(r_{x,y}\\) es la probabilidad de que la cadena alcance el estado \\(y\\) (en algún tiempo futuro) si la cadena se inicia en el estado \\(x\\).\n\n\n\n\n\n\nDefinición\n\n\n\nSean \\(x,y\\in\\mathcal{S}\\) con \\(x\\neq y\\).\n\nDecimos que \\(y\\) es accesible desde \\(x\\) si \\(r_{x,y}&gt;0\\). En tal caso escribimos \\(x\\to y\\).\nDecimos que \\(x\\) se comunica con \\(y\\) si son accesibles entre sí (es decir, \\(x\\to y\\), \\(y\\to x\\)). En tal caso escribimos \\(x\\leftrightarrow y\\).\n\nPor convenio, se considera que cualquier estado \\(x\\) es accesible desde sí mismo (\\(x\\to x\\)), y que se comunica consigo mismo (\\(x\\leftrightarrow x\\)), incluyendo el caso de que \\(r_{x,x}=0\\).\n\n\nCada cadena puede ser representada con un grafo. Podemos ver la accesibilidad simplemente observando si en el grafo existe un camino desde \\(x\\) hasta \\(y\\) respetando la dirección de las flechas. Cuando haya caminos en ambas direcciones, significará que ambos estados se comunican.\nPor ejemplo, consideremos la cadena dada por el grafo en la Figura Figure 3.1. Vemos por ejemplo que \\(1\\to 3\\) ya que podemos ir desde 1 hasta 3, siguiendo la dirección de las flechas, pasando por 4 y 2. Sin embargo, \\(3 \\nrightarrow 1\\), ya que de 3 sólo se puede volver a saltar a sí mismo. Por otro lado, \\(1\\leftrightarrow 2\\) ya que podemos conectar ambos estados por caminos tanto empezando en 1 como empezando en 2.\n\n\n\n\n\nFigure 3.1: Observando las flechas podemos analizar la accesibilidad y la conexión entre estados\n\n\n\n\nEn general, si un estado \\(y\\) es accesible desde un estado \\(x\\), siempre será posible encontrar un camino desde \\(x\\) hasta \\(y\\) de modo que dicho camino no pase por un mismo estado dos veces. Para ello, basta considerar cualquier camino desde \\(x\\) hasta \\(y\\), y si algún estado \\(z\\) aparece dos veces en el camino, eliminamos el tramo del camino desde la primera aparición de dicho estado hasta la última aparición del mismo, de modo que aparezca una sola vez. Dicho camino, al no pasar dos veces por un mismo estado, tendrá como mucho tantos pasos como estados tenga la cadena. En definitiva, tenemos lo siguiente.\n\n\n\n\n\n\nProposición\n\n\n\nSupongamos que el espacio de estados \\(\\mathcal{S}\\) tiene \\(N\\) elementos. Entonces, \\[\nx\\to y\\quad\\mbox{ si, y sólo si, }\\quad p_{x,y}^{(n)}&gt;0\\mbox{ para algún }n\\le N.\n\\]\n\n\nLa relación entre estados definida por la propiedad de estar comunicados es una relación de equivalencia. Es decir, tenemos lo siguiente.\n\n\n\n\n\n\nProposición\n\n\n\nSean \\(x,y\\in\\mathcal{S}\\) dos estados. Se verifican las siguiente propiedades:\n\n\\(x\\leftrightarrow x\\).\nSi \\(x\\leftrightarrow y\\) entonces \\(y\\leftrightarrow x\\).\nSi \\(x\\leftrightarrow y\\) e \\(y \\leftrightarrow z\\), entonces \\(x \\leftrightarrow z\\).\n\n\n\nEn una cadena, siempre podemos agrupar los estados en clases de modo que dos estados en una misma clase estén comunicados y elementos de clases distintas no estén comunicados. Más específicamente, tenemos lo siguiente.\n\n\n\n\n\n\nProposición\n\n\n\nDado el espacio de estados \\(\\mathcal{S}\\), siempre es posible dividirlo en clases disjuntas \\[\n\\mathcal{S}=C_1\\cup C_2\\cup\\ldots\\cup C_n\n\\tag{3.2}\\]\ndonde para todo \\(x,y\\in\\mathcal{S}\\) se verifica \\[x\\leftrightarrow y\\quad\\mbox{ si }x,y\\in C_i,\\] \\[x\\nleftrightarrow y\\quad\\mbox{ si }x\\in C_i\\mbox{, }y\\in C_j,\\: i\\neq j.\\]\n\n\nLas clases en (Equation 3.2) se llaman clases irreducibles. La proposición anterior nos dice que cada cadena admite siempre una descomposición en clases irreducibles. Una cadena se dice que es irreducible si sólo posee una clase irreducible, es decir, todos sus estados se comunican entre sí.\nLas clases irreducibles pueden ser fácilmente identificadas mirando el grafo de la cadena. Observando de nuevo el grafo de la cadena en la Figura Figure 3.1, vemos que \\(1\\to 5\\) pero \\(5 \\nrightarrow 1\\), por lo que están en clases diferentes. Por otro lado, \\(1\\to 4\\), \\(4\\to 2\\), \\(2\\to 1\\), por lo que están en la misma clase. Finalmente, \\(3\\) no está comunicado con nadie, luego forma él solo una clase. En la Figura Figure 3.2, podemos ver las tres clases irreducibles de la cadena que hemos identificado.\n\n\n\n\n\nFigure 3.2: Tenemos en distinto color las tres clases irreducibles\n\n\n\n\nA continuación vamos a introducir un criterio para clasificar los distintos estados de una cadena.\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(x\\in\\mathcal{S}\\).\n\nDecimos que \\(x\\) es recurrente si \\(r_{x,x}=1\\).\nDecimos que \\(x\\) es transitorio si \\(r_{x,x}&lt;1\\).\nDecimos que \\(x\\) es absorbente si \\(p_{x,x}=1\\).\n\n\n\nSi \\(x\\) es recurrente, tendremos que una vez que la cadena alcanza el estado \\(x\\) entonces tendremos total certeza de que volverá a alcanzar el estado \\(x\\) en el futuro.\nSi \\(x\\) es transitorio, tendremos que una vez que la cadena alcanza el estado \\(x\\) entonces no tendremos certeza de que la cadena vuelva a alcanzar el estado \\(x\\) de nuevo.\nSi \\(x\\) es absorbente, tendremos que una vez que la cadena alcanza el estado \\(x\\) con toda certeza permanezca en el estado \\(x\\) en el futuro. En particular, todo estado absorbente es también recurrente.\nMirando de nuevo al grafo de la Figura Figure 3.2 arriba, vemos que el estado 3 es absorbente (y por tanto recurrente), ya que si comenzamos en él sólo llegaremos a él mismo. Los estados 5 y 6 son recurrentes. Por ejemplo, vemos que si empezamos en 5 no podremos llegar a ningún otro estado excepto 6 y él mismo. Además, la única manera para que, empezando en 5, no se vuelva a visitar 5 es que la cadena se cambie al estado 6 y permanezca en ese estado en lo sucesivo. Pero la probabilidad de que eso ocurra es \\(\\frac{1}{2}\\cdot \\frac{1}{2}\\cdot \\frac{1}{2}\\cdot \\ldots=0\\). Así que, con probabilidad 1 se volverá en algún momento a 5, y por lo tanto es recurrente. El mismo argumento es aplicable a 6. Finalmente, los estados 1, 2 y 4 son transitorios ya que desde esos estados se accede a zonas de las que no se puede volver.\nA continuación, enunciamos, sin demostración, algunos resultados sobre el comportamiento de los estados recurrentes.\n\n\n\n\n\n\nTeorema\n\n\n\nSi \\(x\\to y\\) y \\(x\\) es recurrente, entonces \\(y\\) también es recurrente y, además, \\(r_{x,y}=1=r_{y,x}\\).\n\n\nEl teorema de arriba nos dice que si \\(y\\) es accesible desde un estado recurrente, entonces se realizarán viajes de ida y vuelta entre \\(x\\) e \\(y\\), una y otra vez.\n\n\n\n\n\n\nTeorema\n\n\n\nToda cadena de Markov homogénea posee al menos un estado recurrente.\n\n\nComo consecuencia de los dos teoremas anteriores tenemos.\n\n\n\n\n\n\nCorolario\n\n\n\nTodos los estados de una cadena de Markov irreducible son recurrentes. Además, \\(r_{x,y}=1\\) para cualesquiera dos estados \\(x,y\\) de la cadena.\n\n\nEn el caso de cadenas no irreducibles tendremos los siguiente.\n\n\n\n\n\n\nCorolario\n\n\n\nSupongamos que una cadena de Markov tiene descomposición en clases irreducibles \\[ C_1\\cup C_2\\cup \\ldots \\cup C_n. \\] Entonces si existe \\(x\\in C_i\\) el cual es recurrente (resp. transitorio), entonces todos los \\(y\\in C_i\\) son también recurrentes (resp. transitorios).\n\n\nObsérvese que, en la Figura Figure 3.2 arriba, los estados 1, 2 y 4 están todos en una misma clase y son transitorios; los estados 5 y 6 están los dos en una misma clase y son recurrentes; finalmente, el estado 3 es absorbente y formará él solo una clase irreducible.\nEn las cadenas de Markov frecuentemente se observan patrones cíclicos, donde una sucesión de estados se recorre en un camino en forma de bucle empezando y acabando en un mismo estado. Esto da lugar al siguiente concepto.\n\n\n\n\n\n\nDefinición\n\n\n\nUn estado \\(x\\in\\mathcal{S}\\) se dice que tiene periodo \\(d\\) si \\(p^{(n)}_{x,x}=0\\) para todo \\(n\\) que no sea divisible por \\(d\\), y además \\(d\\) es el mayor número con esta propiedad. En el caso de que \\(p^{(n)}_{x,x}=0\\) para todo \\(n\\) (es decir, si \\(r_{x,x}=0\\)), entonces diremos que el periodo de \\(x\\) es infinito. Un estado \\(x\\) con periodo 1 se dice que es aperiódico.\n\n\nEn otras palabras, un estado \\(x\\) tiene periodo \\(d\\) si la cadena puede volver al estado \\(x\\) sólo en un número de pasos que sea múltiplo de \\(d\\). En el caso de que la cadena no vuelva nunca más a \\(x\\), el periodo es infinito.\nUna importante propiedad es la siguiente.\n\n\n\n\n\n\nProposición\n\n\n\nSi \\(x\\leftrightarrow y\\), entonces \\(x\\) e \\(y\\) tienen el mismo periodo.\n\n\nPor definición en una cadena irreducible todos los estados se comunican, por lo que tenemos lo siguiente.\n\n\n\n\n\n\nCorolario\n\n\n\nEn una cadena irreducible todos los estados tienen el mismo periodo.\n\n\nDado que todos los estados de una cadena irreducible tienen el mismo periodo, tiene sentido introducir lo siguiente.\n\n\n\n\n\n\nDefinición\n\n\n\nSe dice que una cadena irreducible tiene periodo \\(d\\) si uno de sus estados (y por tanto todos) tienen periodo \\(d\\). Se dice que una cadena es aperiódica si uno de sus estados (y por tanto todos) es aperiódico.\n\n\nEn el caso de cadenas no irreducibles tendremos los siguiente.\n\n\n\n\n\n\nCorolario\n\n\n\nSupongamos que una cadena de Markov tiene descomposición en clases irreducibles \\[ C_1\\cup C_2\\cup \\ldots \\cup C_n. \\] Todos los estados pertenecientes a una misma clase \\(C_i\\) tienen el mismo periodo.\n\n\nVeremos más adelante que conocer si una cadena irreducible es aperíodica es importante en el estudio límite de las cadenas de Markov. Para determinar si un estado es aperiódico aplicaremos un simple método que explicaremos a continuación.\nRecuerda que dos números n y m son coprimos si su máximo común divisor es 1. Supongamos que podemos encontrar dos números coprimos \\(n\\) y \\(m\\) tal que \\(p_{x,x}^{(n)}&gt;0\\) y \\(p_{x,x}^{(m)}&gt;0\\). En este caso, por definición de periodo, \\(d\\) debe dividir a \\(n\\) y \\(m\\). Como \\(n\\) y \\(m\\) son coprimos, necesariamente tendremos que \\(d=1\\). Por tanto, para determinar si un estado \\(x\\) es aperiódico, trataremos de buscar en el grafo dos caminos de \\(x\\) en sí mismo de modo que los números de pasos dados en sendos caminos sean coprimos.\nPara ilustrar este método, volvamos al ejemplo en la Figura Figure 3.2 arriba. Claramente el estado 3 es aperiódico por ser absorbente. Por otro lado, partiendo del estado 1, podremos volver a dicho estado en tres pasos siguiendo el camino \\(1\\to 4 \\to 2 \\to 1\\). Pero, partiendo de 1, también podemos volver a \\(1\\) en cuatro pasos a través del camino \\(1\\to 4 \\to 4 \\to 2 \\to 1\\) en el que pasamos dos veces por 4. Como 3 y 4 son coprimos, necesariamente tendremos que el estado 1 es aperiódico. Además, los estados 2 y 4 también serán aperiódicos por estar en la misma clase irreducible que 1. Un argumento similar muestra que los estados 5 y 6, los cuales están en la misma clase irreducible, son aperiódicos también."
  },
  {
    "objectID": "Tema2.html#número-de-visitas",
    "href": "Tema2.html#número-de-visitas",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.4 Número de visitas",
    "text": "3.4 Número de visitas\nEn esta sección vamos a estudiar el número medio de veces que una cadena de Markov alcanza un estado determinado a lo largo del tiempo. En lo que sigue consideramos una cadena \\((X_n)_{n=0,1,2,\\ldots}\\) con espacio de estados \\(\\mathcal{S}\\). Dado \\(x\\in\\mathcal{S}\\), la probabilidad de que la cadena vuelva a \\(x\\) habiendo empezando en \\(x\\) es \\(r_{x,x}\\). Por tanto, si empezamos en \\(x\\), la probabilidad de no volver nunca más a \\(x\\) es \\(1-r_{x,x}\\). Teniendo en cuenta ambas cosas, tenemos que \\[\n\\mathbb{P}(\\mbox{``visitar $x$ exactamente $k$ veces''}|X_0=x)=r_{x,x}^k(1-r_{x,x}),\n\\tag{3.3}\\] donde no estamos contando la visita a \\(x\\) en tiempo \\(0\\).\n\n\n\n\n\n\nComentario\n\n\n\nRecordemos que una variable aleatoria discreta \\(Y\\) sigue una distribución geométrica de parámetro \\(p\\in [0,1]\\) si \\[ \\mathbb{P}(Y=k)=(1-p)^{k}p\\quad\\mbox{ para }k=0,1,2,3\\ldots \\]\nEn ese caso, \\[ \\mathbb{E}(Y)=\\frac{1-p}{p}. \\] Obsérvese que la anterior esperanza es infinita si \\(p=0\\). Para los demás valores de \\(p\\) tendremos valores finitos.\n\n\nA partir de (Equation 3.3), vemos que el número de visitas a \\(x\\) partiendo de \\(x\\) sigue una distribución Geométrica de parámetro \\(p=1-r_{x,x}\\). En particular, el número esperado de visitas a \\(x\\) si salimos de \\(x\\) es \\[ \\mathbb{E}(\\mbox{``número de visitas a $x$''}\\mid X_0=x)=\\frac{r_{x,x}}{1-r_{x,x}}. \\] Además, la expresión arriba es finita si, y sólo si, \\(r_{x,x}&lt;1\\), en cuyo caso \\(x\\) es transitorio. En caso contrario, tendremos un estado recurrente y el número esperado de visitas será infinito.\nConsideremos ahora un estado \\(y\\) distinto de \\(x\\), de modo que \\(y\\) es accesible desde \\(x\\). Queremos calcular ahora el número medio de visitas a \\(y\\) empezando en \\(x\\). Si partimos de \\(x\\), tenemos dos posibilidades, o bien la cadena alcanza el estado \\(y\\) produciendose al menos una visita, lo cual ocurre con probabilidad \\(r_{x,y}\\); o bien no lo visita nunca, lo cual ocurre con probabilidad \\(1-r_{x,y}\\). De esta manera, el número esperado de visitas a \\(y\\) si salimos de \\(x\\) es \\[\\begin{align*}\n\\mathbb{E}(\\mbox{``número de visitas a $y$''}|X_0=x)&=r_{x,y}\\cdot\\Big(1+ \\mathbb{E}(\\mbox{``número de visitas a $y$ con $n&gt;1$''}|X_1=y)\\Big) + (1-r_{x,y})\\cdot 0\\\\\n&=r_{x,y}\\left(1+ \\frac{r_{y,y}}{1-r_{y,y}}\\right)\\\\\n&=\\frac{r_{x,y}}{1-r_{y,y}}.\n\\end{align*}\\] De nuevo podemos observar que la esperanza anterior es finita si, y sólo si, \\(r_{y,y}&lt;1\\), en cuyo caso \\(y\\) es transitorio.\nResumimos los resultados obtenidos en el siguiente enunciado.\n\n\n\n\n\n\nProposición\n\n\n\nSupongamos que \\(x,y\\) son dos estados (posiblemente iguales) de modo que \\(x\\to y\\). Entonces, \\[ \\mathbb{E}(\\mbox{``número de visitas a $y$''}\\mid X_0=x)=\\frac{r_{x,y}}{1-r_{y,y}}. \\] Además, la esperanza anterior es finita si y solo si \\(y\\) es transitorio."
  },
  {
    "objectID": "Tema2.html#probabilidades-de-absorción-y-tiempos-medios-de-llegada",
    "href": "Tema2.html#probabilidades-de-absorción-y-tiempos-medios-de-llegada",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.5 Probabilidades de absorción y tiempos medios de llegada",
    "text": "3.5 Probabilidades de absorción y tiempos medios de llegada\nCuando se estudian las cadenas de Markov, es habitual preguntarse por la probabilidad de que la cadena alcance cierto estado absorbente y también por el tiempo medio en el que se alcanza dicho estado. Por ejemplo, en el caso de la ruina del jugador, podríamos plantearnos cómo de probable es alcanzar las ganancias que el jugador ha fijado antes de retirarse, cuál es la probabilidad de arruinarse, o cuánto tiempo tardarán en producirse dichos sucesos. A continuación, introduciremos los conceptos de probabilidad de absorción y tiempo medio de llegada, y veremos cómo se calculan.\n\n3.5.1 Probabilidades de absorción\nSupongamos que tenemos una cadena con todos sus estados transitorios o absorbentes. Por ejemplo, supongamos que tenemos la cadena dada el siguiente grafo:\n\n\n\n\n\n\n\n\n\nEsta cadena tiene matriz de transición \\[ P= \\left[\n\\begin{array}{cccc}\n1 & 0 & 0 & 0\\\\\n1/3 & 0 & 2/3 & 0\\\\\n0 & 1/2 & 0 & 1/2\\\\\n0 & 0 & 0 & 1\n\\end{array}\n\\right]. \\]\nPor otro lado, en dicha cadena encontramos tres clases irreducibles: En primer lugar una clase \\(C_1\\) formada por sólo el estado absorbente 1, una clase \\(C_2\\) con los estados transitorios 2 y 3, y una clase \\(C_3\\) con sólo el estado absorbente 4.\nNos planteamos calcular las probabilidades de que cualquier estado sea absorbido por el estado absorbente 1. En concreto, dependiendo de que la cadena de Markov se encuentre en 1, 2, 3, o 4, las probabilidades de absorción en 1 vendrán dadas por \\(r_{1,1}\\), \\(r_{2,1}\\), \\(r_{3,1}\\), o \\(r_{4,1}\\), respectivamente.\nClaramente, \\(r_{1,1}=1\\) y \\(r_{4,1}=0\\). Para encontrar \\(r_{2,1}\\) procedemos como sigue. Puesto que desde el estado 2 se accede, en un solo paso, al estado 1 o al estado 3, tendremos\n\\[ r_{2,1}=p_{2,1}r_{1,1} + p_{2,3}r_{3,1}=\\frac{1}{3}r_{1,1}+\\frac{2}{3}r_{3,1}. \\] Por otro lado, para hallar \\(r_{3,1}\\), usamos que desde el estado 3 se accede, en un solo paso, al estado 4 o al estado 2. De este modo \\[ r_{3,1}=p_{3,4}r_{4,1} + p_{3,2}r_{2,1}=\\frac{1}{2}r_{4,1}+\\frac{1}{2}r_{2,1}. \\] Poniendo todo junto, tenemos el siguente sistema de ecuaciones \\[\n\\begin{cases}\nr_{1,1}&=1,\\\\\nr_{2,1}&=\\frac{1}{3}r_{1,1}+\\frac{2}{3}r_{3,1},\\\\\nr_{3,1}&=\\frac{1}{2}r_{4,1}+\\frac{1}{2}r_{2,1},\\\\\nr_{4,1}&=0.\n\\end{cases}\n\\] Resolviendo el sistema anterior obtenemos \\[ r_{1,1}=1,\\quad r_{2,1}=\\frac{1}{2},\\quad r_{3,1}=\\frac{1}{4},\\quad r_{4,1}=0. \\]\nCon una argumentación similar, es posible encontrar las probabilidades \\(r_{1,4}\\), \\(r_{2,4}\\), \\(r_{3,4}\\), o \\(r_{4,4}\\) de ser absorbido en el estado 4 si la cadena se encuentra en 1, 2, 3, o 4, respectivamente. ¿Puedes hallar dichas probabilidades?\nCon una argumentación similar, es posible encontrar las probabilidades \\(r_{1,4}\\), \\(r_{2,4}\\), \\(r_{3,4}\\), o \\(r_{4,4}\\) de ser absorbido en el estado 4 si la cadena se encuentra en 1, 2, 3, o 4, respectivamente. ¿Puedes hallar dichas probabilidades?\nEn general, si \\((X_n)_{n=0,1,2,\\ldots}\\) es una cadena con todos sus estados absorbentes o transitorios, entonces las probabilidades de absorción de un estado absorbente \\(a\\in\\mathcal{S}\\) se pueden calcular usando que:\n\n\\(r_{a,a}=1\\),\n\\(r_{x,a}=0\\) para cualquier otro estado absorbente \\(x\\in\\mathcal{S}\\),\npara cualquier estado transitorio \\(x\\in\\mathcal{S}\\),\n\n\\[\n    r_{x,a}=\\sum_{y\\in \\mathcal{S}} p_{x,y} r_{y,a}.\n\\]\n\n\n3.5.2 Tiempos medios de llegada\nSupongamos que tenemos ahora una cadena con un solo estado absorbente y todos los demás transitorios. Por ejemplo, consideremos la cadena dada por el grafo\n\n\n\n\n\n\n\n\n\nDefinimos \\(\\tau_{1,3}\\) como el número de pasos promedio necesarios para alcanzar estado absorbente 3 partiendo de 1, es decir, \\[\n\\tau_{1,3}=\\mathbb{E}[\\mbox{``Número de pasos hasta la primera visita a $3$''}|X_0=1].\n\\] Del mismo modo, podemos definir \\(\\tau_{2,3}\\) y \\(\\tau_{3,3}\\).\nObviamente \\(\\tau_{3,3}=0\\). Para calcular \\(\\tau_{1,3}\\), tenemos que tener en cuenta que, si la cadena se encuentra en el estado \\(1\\), en el primer paso dado o bien vuelve al estado 1, o bien va al estado 2. De esa manera, \\[\n\\tau_{1,3}=1+p_{1,1}\\tau_{1,3} +p_{1,2}\\tau_{2,3}=1+\\frac{1}{2}\\tau_{1,3} +\\frac{1}{2}\\tau_{2,3}.\n\\] Si estamos en el estado 2, daremos el primer paso o bien a 1, o bien a 3. Luego, \\[\n\\tau_{2,3}=1+p_{2,1}\\tau_{1,3} +p_{2,3}\\tau_{3,3}=1+\\frac{1}{3}\\tau_{1,3} +\\frac{2}{3}\\tau_{3,3}.\n\\] Poniendo todo junto, obtenemos el siguiente sistema de ecuaciones \\[\n\\begin{cases}\n\\tau_{1,3}&=1+\\frac{1}{2}\\tau_{1,3} +\\frac{1}{2}\\tau_{2,3},\\\\\n\\tau_{2,3}&=1+\\frac{1}{3}\\tau_{1,3} +\\frac{2}{3}\\tau_{3,3},\\\\\n\\tau_{3,3}&=0.\n\\end{cases}\n\\] Resolviendo el sistema anterior, obtenemos \\[\n\\tau_{1,3}=\\frac{9}{2},\\quad\n\\tau_{2,3}=\\frac{5}{2},\\quad\n\\tau_{3,3}=0.\n\\]\nEn general, si \\((X_n)_{n=0,1,2,\\ldots}\\) una cadena con un solo estado absorbente \\(a\\in\\mathcal{S}\\) y todos los demás estados transitorios, entonces los tiempos medios de llegada a \\(a\\in\\mathcal{S}\\) se pueden calcular usando que:\n\n\\(\\tau_{a,a}=0\\),\npara cualquier estado \\(x\\in\\mathcal{S}\\) distinto de \\(a\\), \\[\n\\tau_{x,a}=1+\\sum_{y\\in \\mathcal{S}} p_{x,y} \\tau_{y,a}.\n\\]"
  },
  {
    "objectID": "Tema2.html#comportamiento-asintótico",
    "href": "Tema2.html#comportamiento-asintótico",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.6 Comportamiento asintótico",
    "text": "3.6 Comportamiento asintótico\nComo hemos visto, en una cadena irreducible, todos los estados son recurrentes. Tenemos que es posible pasar de cualquier estado a cualquier otro. De hecho la cadena pasará infinitas veces por todos los estado con probabilidad 1. Nos preguntamos sobre cuál es la probabilidad de que a largo plazo la cadena se encuentre en un estado particular. En otras palabras, queremos saber con qué frecuencia la cadena visitará dicho estado a medida que avanzamos en el tiempo.\nEn lo que sigue, consideramos una cadena \\((X_n)_{n=0,1,2,\\ldots}\\) irreducible con espacio de estados \\(\\mathcal{S}=\\{1,2,\\ldots,N\\}\\).\nPara cada \\(n=0,1,2,\\ldots\\) y \\(j\\in \\mathcal{S}\\) consideramos la probabilidad de que la cadena se encuentre en el estado \\(j\\) en \\(n\\) pasos, es decir, \\[ \\pi^{(n)}_j=\\mathbb{P}(X_n=j). \\] De hecho podemos considerar el vector \\[ \\pi^{(n)}=(\\pi{(n)}_1, \\pi^{(n)}_2,\\ldots, \\pi^{(n)}_N). \\] Al considerar la probabilidad de \\(n\\) pasos para todos los posibles estados, el vector \\(\\pi^{(n)}\\) verificará además que sus componentes suman \\(1\\), es decir, \\(\\sum_{j=1}^N \\pi^{(n)}_j=1\\).\nDado un estado \\(j\\in\\mathcal{S}\\), teniendo en cuenta la probabilidad de empezar en cada uno de los estados, tendremos que \\[\\begin{align*}\n\\pi^{(n)}_j&=\\sum_{i=1}^N \\mathbb{P}(X_0=i)\\cdot \\mathbb{P}(X_n=j|X_0=i)\\\\\n&=\\sum_{i=1}^N \\pi^{(0)}_i\\cdot p^{(n)}_{i,j}.\n\\end{align*}\\] Lo cual conduce a la siguiente ecuación matricial \\[\n\\begin{aligned}\n\\pi^{(n)}=\\pi^{(0)} \\cdot P^n.\n\\end{aligned}\n\\tag{3.4}\\]\nSupongamos que existe el límite \\(\\lim_{n\\to\\infty}\\pi^{(n)}=\\pi\\). El vector \\(\\pi=(\\pi_1,\\pi_2,\\ldots,\\pi_N)\\) se llama distribución límite de la cadena de Markov. Puesto que \\(\\sum_{j=1}^N \\pi^{(n)}_j=1\\), la distribución límite necesariamente verificará \\[\n\\sum_{j=1}^N \\pi_j=1.\n\\] Por otro lado, usando (Equation 3.4), tenemos \\[\n\\begin{aligned}\n\\pi&=\\lim_{n\\to\\infty}\\pi^{(n+1)}\\\\\n&=\\lim_{n\\to\\infty}( \\pi^{(0)} \\cdot P^{n+1})\\\\\n&=(\\lim_{n\\to\\infty} \\pi^{(0)} \\cdot P^{n})\\cdot P \\\\\n&=(\\lim_{n\\to\\infty}\\pi^{(n)}) \\cdot P\\\\\n&=\\pi\\cdot P.\n\\end{aligned}\n\\tag{3.5}\\]\nLa distribución límite \\(\\pi\\) cumple la ecuación matricial \\[\n\\begin{aligned}\n\\pi=\\pi\\cdot P.\n\\end{aligned}\n\\tag{3.6}\\]\nPodemos interpretar esta ecuación de manera intuitiva de la siguiente forma. Supongamos que en un instante de tiempo \\(n\\) la variable aleatoria \\(X_n\\) está distribuida según \\(\\pi\\). Al dar un paso en la cadena, la distribución de \\(X_{n+1}\\) será \\(\\pi P\\). Ahora bien, si se cumple que \\(\\pi=\\pi P\\), entonces \\(X_{n+1}\\) sigue teniendo la distribución \\(\\pi\\). Es decir, tras la transición la distribución no cambia. Esto significa que la cadena de Markov ha alcanzado una distribución estable en el tiempo. A una distribución \\(\\pi\\) verificando (Equation 3.6) se llama distribución estacionaria de la cadena de Markov.\nHemos argumentado arriba que la distribución límite, si existe, debe cumplir las ecuaciones (Equation 3.5) y (Equation 3.6). Por otra parte, es posible demostrar que si una cadena irreducible es aperiódica entonces la distribución límite existe. Por tanto, tenemos lo siguiente.\n\n\n\n\n\n\nTeorema\n\n\n\nSupongamos que \\((X_n)_{n=0,1,2,\\ldots}\\) es una cadena irreducible aperiódica. Entonces, la distribución límite existe y está determinada por las ecuaciones \\[\n\\begin{cases}\n\\pi=\\pi P,\\\\\n\\sum_{j=1}^N \\pi_j=1.\n\\end{cases}\n\\]\n\n\nDe esta manera, para encontrar la distribución límite de una cadena, bastará con comprobar que la cadena es irreducible y aperiódica, lo cual garantiza la existencia de una distribución límite, y luego resolver las ecuaciones anteriores para determinarla.\n\n\n\n\n\n\nEjemplo\n\n\n\nConsideremos una cadena de Markov con sólo dos estados \\(\\mathcal{S}=\\{1,2\\}\\), y con matriz de transición \\[ P=\\left[\n\\begin{array}{cc}\n\\frac{2}{3} & \\frac{1}{3}\\\\\n\\frac{1}{2} & \\frac{1}{2}\n\\end{array}\n\\right]. \\] Esta cadena es irreducible y aperiódica ya que todos los estados están comunicados en un solo paso. Por tanto, existirá la distribución límite \\(\\pi=(\\pi_1,\\pi_2)\\), la cual será también una distribución estacionaria. Para hallar dicha distribución, hemos de resolver la ecuación matricial \\[ \\left[\\begin{array}{cc}\\pi_1 & \\pi_2 \\end{array}\\right]\\left[\n\\begin{array}{cc}\n\\frac{2}{3} & \\frac{1}{3}\\\\\n\\frac{1}{2} & \\frac{1}{2}\n\\end{array}\n\\right]=\\left[\\begin{array}{cc}\\pi_1 & \\pi_2 \\end{array}\\right], \\] junto con la condición \\(\\pi_1+\\pi_2=1\\). Esto nos conduce al sistema de ecuaciones:\n\\[\n\\begin{cases}\n\\frac{1}{3}\\pi_1 - \\frac{1}{2}\\pi_2&=0,\\\\\n-\\frac{1}{3}\\pi_1 + \\frac{1}{2}\\pi_2&=0,\\\\\n\\pi_1+\\pi_2&=1.\n\\end{cases}\n\\] Resolviendo obtenemos la distribución límite \\[ \\pi_1=\\frac{3}{5}=0.6,\\quad \\pi_2=\\frac{2}{5}=0.4. \\]\nDicha distribución límite es posible aproximarla por “fuerza bruta” tomando potencias sucesivas de la matriz de transición. Lo haremos con ayuda de R:\n\nP &lt;- rbind(c(2/3, 1/3),\n           c(1/2, 1/2))\nP2 &lt;- P%*%P\nP3 &lt;- P2%*%P\nP4 &lt;- P3%*%P\nP5 &lt;- P4%*%P\nP6 &lt;- P5%*%P\nP7 &lt;- P6%*%P\nP8 &lt;- P7%*%P\nP9 &lt;- P8%*%P\nP10 &lt;- P9%*%P\n\nP\n\n          [,1]      [,2]\n[1,] 0.6666667 0.3333333\n[2,] 0.5000000 0.5000000\n\nP2\n\n          [,1]      [,2]\n[1,] 0.6111111 0.3888889\n[2,] 0.5833333 0.4166667\n\nP3\n\n          [,1]      [,2]\n[1,] 0.6018519 0.3981481\n[2,] 0.5972222 0.4027778\n\nP4\n\n          [,1]      [,2]\n[1,] 0.6003086 0.3996914\n[2,] 0.5995370 0.4004630\n\nP5\n\n          [,1]      [,2]\n[1,] 0.6000514 0.3999486\n[2,] 0.5999228 0.4000772\n\nP6\n\n          [,1]      [,2]\n[1,] 0.6000086 0.3999914\n[2,] 0.5999871 0.4000129\n\nP7\n\n          [,1]      [,2]\n[1,] 0.6000014 0.3999986\n[2,] 0.5999979 0.4000021\n\nP8\n\n          [,1]      [,2]\n[1,] 0.6000002 0.3999998\n[2,] 0.5999996 0.4000004\n\nP9\n\n          [,1]      [,2]\n[1,] 0.6000000 0.4000000\n[2,] 0.5999999 0.4000001\n\nP10\n\n     [,1] [,2]\n[1,]  0.6  0.4\n[2,]  0.6  0.4\n\n\nVemos que al tomar sucesivas potencias de la matriz de transición, el valor de la matriz se va estabilizando y sus filas se van aproximando a la distribución estacionaria \\(\\pi\\) que hemos obtenido anteriormente. Recordemos que la potencia n-ésima de la matriz de transición corresponde a la matriz de transición de n pasos. Luego es lógico que al ir tomando potencias de la matriz de transición, obtengamos valores cada vez más cercanos a la distribución límite. Vemos también que la probabilidad límite no depende del valor inicial de la cadena, ya que todas las filas convergen a los mismos valores."
  },
  {
    "objectID": "Tema2.html#algoritmo-pagerank",
    "href": "Tema2.html#algoritmo-pagerank",
    "title": "3  Tema 2. Cadenas de Markov",
    "section": "3.7 Algoritmo PageRank",
    "text": "3.7 Algoritmo PageRank\nVolvamos al ejemplo que vimos al principio del tema donde analizamos la cadena de Markov del algoritmo PageRank de Google. Recordemos que dicha cadena describía a un “surfeador de internet” que pulsaba al azar los links de un conjunto de páginas. El algoritmo PageRank se usa para ordenar la aparición de las páginas en cada búsqueda en Google, dando preferencia a aquellas páginas cuya probabilidad sea mayor en la distribución límite. Es decir, aquellas con mayor probabilidad serán las que sean más visitadas a largo plazo.\nEn nuestro ejemplo, el “surfeador de la web” navega al azar entre las páginas A, B, C y D, donde:\n\nA tiene enlace a B,\nB tiene enlaces a A y C,\nC tiene enlace a A,\nD tienes enlaces a las otras tres páginas.\n\nDe esta manera, el espacio de estados es \\(\\{A,B,C,D\\}\\), y la matriz de transición será \\[ P= \\left[\n\\begin{array}{cccc}\n0 & 1 & 0 & 0\\\\\n1/2 & 0 & 1/2 & 0\\\\\n1 & 0 & 0 & 0\\\\\n1/3 & 1/3 & 1/3 & 0\n\\end{array}\n\\right]. \\] Como hemos dicho, estamos interesados en determinar la distribución límite de la cadena de Markov definida por el surfeador. El problema es que dicha cadena podría no ser irreducible, por lo que la distribución límite podría no existir. De hecho, en nuestro ejemplo en particular, la cadena no es irreducible ya que, si bien todas las páginas son accesibles desde D, dicha página no es accesible desde el resto de las páginas.\nPara solucionar este problema, el algoritmo de PageRank supone que, en cualquier paso, el surfeador de la web se puede “aburrir”, y empezar a navegar de nuevo en cualquiera de las páginas del conjunto al azar. La probabilidad con la que, tras cada paso, el surfeador se aburre es un número fijo \\(p\\). A la probabilidad complementaria \\(d=1-p\\) se le llama factor de amortiguamiento (damping factor). Por regla general se suele tomar factor de amortiguamiento \\(d=0.85\\).\nEn nuestro caso particular, si el surfeador se encuentra, por ejemplo, en la página \\(B\\) podría seguir navegando y ir a cualquiera de las páginas a las que tiene enlace, es decir, A o C, o podría aburrirse y saltar a cualquiera de las páginas del conjunto A,B,C, o D.\nLa cadena de Markov así obtenida es irreducible ya que desde cada página se puede pasar a cualquier otra página ya que existe la posibilidad de aburrirse. Además, será aperiódica ya que dichos saltos pueden darse en un solo paso. Al ser una cadena irreducible y aperiódica tendrá una distribución límite.\nDesde un punto de vista más práctico, añadir la posibilidad de que el surfeador se pueda aburrir, hace que éste no pueda quedarse “atrapado” en un grupo de páginas particular conectadas entre sí, pero no a las demás, haciendo que el algoritmo desestime otras páginas que también podrían ser importantes.\nEn general, supongamos que el surfeador navega al azar entre \\(n\\) páginas con matriz de transición \\(P\\). Entonces, al considerar un factor de amortiguación \\(d\\) obtenemos una nueva matriz de transición la cual viene dada por: \\[\nM=d\\cdot P + (1-d)\\cdot \\frac{1}{n} J_n,\n\\tag{3.7}\\] donde \\(J_n\\) es la matriz cuadrada de tamaño \\(n\\) con todas sus entradas igual a 1. La nueva matriz de transición \\(M\\) representa que, en cada paso, con probabilidad \\(d\\) el surfeador actuará de acuerdo a la cadena con matriz de transición \\(P\\), y con probabilidad \\(1-d\\) el surfeador actuará de acuerdo a la cadena consisten en elegir cualquiera de las n páginas al azar, cuya matriz de transición es \\(\\dfrac{1}{n}J_n\\).\nEn nuestro ejemplo, tomando \\(d=0.85\\), calculamos con ayuda de R la matriz de transición:\n\nP &lt;- rbind(c(0, 1, 0, 0),\n           c(1/2, 0, 1/2, 0),\n           c(1, 0, 0, 0),\n           c(1/3, 1/3, 1/3, 0))\n\nd &lt;- 0.85\nn &lt;- 4\nM &lt;- d*P + (1-d)*(1/n)*matrix(1, n, n)\n\nObtenemos \\[ M= \\left[\n\\begin{array}{cccc}\n0.0375  & 0.8875 & 0.0375 & 0.0375\\\\\n0.4625 & 0.0375 & 0.4625 & 0.0375\\\\\n0.8875 & 0.0375 & 0.0375 & 0.0375\\\\\n0.3208333 & 0.3208333 & 0.3208333 & 0.0375\n\\end{array}\n\\right]. \\] Sabemos que la cadena dada por la anterior matriz de transición posee distribución límite. Podemos calcular su valor exacto resolviendo las ecuaciones correspondientes (veremos en las prácticas cómo hacerlo fácilmente en R). Sin embargo, por comodidad aproximaremos la distribución límite mediante el cálculo de potencias de la matriz de transición. Para ello podemos implementar el siguiente método iterativo en R:\n\nM_nueva &lt;- M\n\n# Fijamos un error maximo \nerror_max &lt;- 10^-15\n\n# Inicializamos la variable de error\nerror &lt;- 1\n\nwhile (error&gt; error_max) {\n  M_nueva &lt;- M_nueva%*%M\n  error &lt;- max(abs(M_nueva - M_nueva%*%M))\n}\n\n# La solución es \nM_nueva[1,]\n\n[1] 0.3824972 0.3732476 0.2067552 0.0375000\n\n\nObtenemos: \\[ \\pi_A=0.3824972,\\quad \\pi_B=0.3732476,\\quad \\pi_C=0.2067552,\\quad \\pi_D=0.0375. \\] Esto quiere decir que, a largo plazo, el surfeador gastará el 38.25% del tiempo en la página A; 37.32% del tiempo en la página B; 20.68% del tiempo en la página C; y el 3.75% del tiempo en la página D.\nPor tanto, el algoritmo PageRank ordenará las páginas en función de estas probabilidades, mostrando las páginas en el orden: A, B, C, D."
  }
]